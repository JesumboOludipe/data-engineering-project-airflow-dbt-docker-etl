# ELT Pipeline with Docker, Airflow, dbt & PostgreSQL

This repository contains a **custom Extract, Load, Transform (ELT)** project that demonstrates a modular data pipeline using **Docker, PostgreSQL, Airflow, dbt**, and **cron jobs**. It showcases how multiple containers and tools can work together to simulate a real-world data engineering workflow.

---

## ğŸš€ Project Overview

The pipeline:

* Extracts data from a source PostgreSQL database
* Loads it into a destination PostgreSQL database
* Applies transformations using **dbt**
* Uses **cron jobs** and **Airflow** for automation and orchestration

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ airflow/                  # Airflow DAGs and configs
â”œâ”€â”€ custom_postgres/         # Custom PostgreSQL configs (if any)
â”œâ”€â”€ elt_script/
â”‚   â”œâ”€â”€ Dockerfile           # Python environment for ELT
â”‚   â””â”€â”€ elt_script.py        # Python script that performs ELT
â”œâ”€â”€ logs/                    # Log files
â”œâ”€â”€ source_db_init/
â”‚   â””â”€â”€ init.sql             # SQL script to seed source PostgreSQL
â”œâ”€â”€ Dockerfile               # Base Dockerfile
â”œâ”€â”€ docker-compose.yaml      # Docker Compose setup
â”œâ”€â”€ elt.sh                   # Shell script to trigger ELT
â”œâ”€â”€ start.sh                 # Starts the containers
â”œâ”€â”€ stop.sh                  # Stops and cleans up
â””â”€â”€ README.md
```

---

## âš™ï¸ How It Works

### 1. **Container Orchestration**

Using `docker-compose.yaml`, three services are deployed:

* `source_postgres`: PostgreSQL container initialized with sample data
* `destination_postgres`: Empty PostgreSQL container (ELT target)
* `elt_script`: Runs Python-based ELT process

### 2. **ELT Process**

* **Extract**: `elt_script.py` connects to `source_postgres` and dumps the database using `pg_dump`
* **Load**: The dump is loaded into `destination_postgres` using `psql`
* **Transform**: dbt models (in `dbt/`) transform the loaded data

### 3. **Database Initialization**

* `init.sql` initializes the `source_postgres` container with tables:

  * `users`, `films`, `categories`, `actors`, and relationships
* Includes sample data for simulation

---

## â±ï¸ Automation with Cron

A **cron job** is configured to run the ELT script at scheduled intervals:

* Keeps the destination database updated automatically
* Useful for simulating regular data ingestion cycles

---

## ğŸŒ Airflow Orchestration

In an extended version, **Apache Airflow** is used to:

* Schedule and monitor ELT jobs
* Chain tasks (e.g., extract â†’ load â†’ transform) with DAGs
* Integrate with `dbt` and external sources (e.g., Aibyte)

---

## ğŸ“ Requirements

* Docker + Docker Compose
* Python 3.x (within containers)
* PostgreSQL client tools (`pg_dump`, `psql`)
* dbt CLI (in future updates)

---

## â–¶ï¸ Running the Project

```bash
# Start the pipeline
bash start.sh

# Run the ELT manually
bash elt.sh

# Stop and clean up
bash stop.sh
```


